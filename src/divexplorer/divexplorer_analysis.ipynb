{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db905f0a",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "powered-information",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T15:20:44.908897Z",
     "start_time": "2022-04-20T15:20:44.883774Z"
    },
    "id": "powered-information"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-doctor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T15:20:45.349495Z",
     "start_time": "2022-04-20T15:20:44.910825Z"
    },
    "id": "built-doctor"
   },
   "outputs": [],
   "source": [
    "import divexplorer \n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n",
    "import os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils_analysis import plotMultipleSV, plotShapleyValue\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c80058f0",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26247e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for sorting data cohorts\n",
    "def sortItemset(x, abbreviations={}):\n",
    "    x = list(x)\n",
    "    x.sort()\n",
    "    x = \", \".join(x)\n",
    "    for k, v in abbreviations.items():\n",
    "        x = x.replace(k, v)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e539fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_in_itemset(itemset, attributes, alls = True):\n",
    "    \"\"\" Check if attributes are in the itemset (all or at least one)\n",
    "    \n",
    "    Args:\n",
    "        itemset (frozenset): the itemset\n",
    "        attributes (list): list of itemset of interest\n",
    "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
    "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Avoid returning the empty itemset (i.e., info of entire dataset)\n",
    "    if itemset == frozenset() and attributes:\n",
    "        return False\n",
    "    \n",
    "    for item in itemset:\n",
    "        # Get the attribute\n",
    "        attr_i = item.split(\"=\")[0]\n",
    "        \n",
    "        #If True, check if ALL attributes of the itemset are the input attributes.\n",
    "        if alls:\n",
    "            # Check if the attribute is present. If not, the itemset is not admitted\n",
    "            if attr_i not in attributes:\n",
    "                return False\n",
    "        else:\n",
    "            # Check if least one attribute. If yes, return True\n",
    "            if attr_i in attributes:\n",
    "                return True\n",
    "    if alls:\n",
    "        # All attributes of the itemset are indeed admitted\n",
    "        return True\n",
    "    else:\n",
    "        # Otherwise, it means that we find None\n",
    "        return False\n",
    "    \n",
    "def filter_itemset_df_by_attributes(df: pd.DataFrame, attributes: list, alls = True, itemset_col_name: str = \"itemsets\") -> pd.DataFrame:\n",
    "    \"\"\"Get the set of itemsets that have the attributes in the input list (all or at least one)\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the input itemsets (with their info). \n",
    "        attributes (list): list of itemset of interest\n",
    "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
    "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
    "        itemset_col_name (str) : the name of the itemset column, \"itemsets\" as default\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: the set of itemsets (with their info)\n",
    "    \"\"\"\n",
    "\n",
    "    return df.loc[df[itemset_col_name].apply(lambda x: attributes_in_itemset(x, attributes, alls = alls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d02de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define abbreviations for plot and visualization\n",
    "from divexplorer.FP_Divergence import abbreviateDict\n",
    "abbreviations = {'Self-reported fluency level=native': 'fluency=native', \\\n",
    "                  'total_silence':'tot_silence', 'location': 'loc', \\\n",
    "                  'Current language used for work/school=English (United States)': 'lang=EN_US', \\\n",
    "                  'ageRange': 'age', \\\n",
    "                  'speakerId' : 'spkID', \\\n",
    "                  'First Language spoken=English (United States)':  'lang=EN_US', \\\n",
    "                  'trimmed': 'trim', \\\n",
    "                  'total_': 'tot_', \\\n",
    "                  'speed_rate_word':'speakRate', \\\n",
    "                  'speed_rate_char':'speakCharRate', \\\n",
    "                  'change language': 'change lang', \\\n",
    "                  'duration': 'dur'}\n",
    "\n",
    "abbreviations_shorter = abbreviations.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "occupational-madrid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T15:07:23.652910Z",
     "start_time": "2022-04-20T15:07:23.612488Z"
    },
    "id": "occupational-madrid"
   },
   "source": [
    "# Define targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461193a2",
   "metadata": {
    "id": "461193a2"
   },
   "outputs": [],
   "source": [
    "## Target for DivExplorer: \n",
    "# 'prediction' is 1 if predicted_label == original_label, 0 otherwise\n",
    "target_col = 'prediction' \n",
    "target_metric = 'd_posr'\n",
    "target_div = 'd_accuracy'\n",
    "t_value_col = 't_value_tp_fn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efa3b44",
   "metadata": {
    "id": "8efa3b44"
   },
   "outputs": [],
   "source": [
    "## Columns for visualization\n",
    "show_cols = ['support', 'itemsets', '#errors', '#corrects', 'accuracy', \\\n",
    "                'd_accuracy', 't_value', 'support_count', 'length']\n",
    "remapped_cols = {'tn': '#errors', 'tp': '#corrects', 'posr': 'accuracy', \\\n",
    "                target_metric: target_div, 't_value_tp_fn': 't_value'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbd447",
   "metadata": {},
   "source": [
    "# FSC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e257bab",
   "metadata": {},
   "source": [
    "## Retrieve Data and Compute Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d73fbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
    "from divexplorer.FP_Divergence import FP_Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63b043ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns of the df file that we are going to analyze \n",
    "demo_cols = ['Self-reported fluency level ', 'First Language spoken',\n",
    "       'Current language used for work/school', 'gender', 'ageRange']\n",
    "\n",
    "slot_cols = ['action', 'object', 'location']\n",
    "\n",
    "signal_cols = ['total_silence', 'total_duration', 'trimmed_duration', \n",
    "       'n_words', 'speed_rate_word', 'speed_rate_word_trimmed'] \n",
    "\n",
    "input_cols = demo_cols + signal_cols + slot_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66552fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.03\n",
    "\n",
    "configs = [\n",
    "    \"original\", \n",
    "    \"acq_random\",\n",
    "    \"acq_knn\",\n",
    "    \"acq_clustering\",\n",
    "    \"acq_divexplorer\",\n",
    "    \"acq_supervision\",\n",
    "    \"aug_random\",\n",
    "    \"aug_knn\",\n",
    "    \"aug_clustering\",\n",
    "    \"aug_divexplorer\",\n",
    "    \"reg_random\",\n",
    "    \"reg_knn\",\n",
    "    \"reg_clustering\",\n",
    "    \"reg_divexplorer\",\n",
    "    ]\n",
    "\n",
    "FP_fm_dict = {}\n",
    "fp_divergence_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    input_file_divexplorer = os.path.join(\\\n",
    "        os.getcwd(), \"results\", \"slu\", \"fsc\", config, \"df_test.csv\")\n",
    "    df = pd.read_csv(input_file_divexplorer, index_col=0)\n",
    "\n",
    "    ## Add SpeakerID information if it is present in the df\n",
    "    if \"speakerId\" in input_cols:\n",
    "        df['speakerId'] = df.index.map(lambda x: x.split(\"/\")[2])\n",
    "\n",
    "    ## Discretize the dataframe\n",
    "    from divergence_utils import discretize\n",
    "\n",
    "    df_discretized = discretize(\n",
    "        df[input_cols+[target_col]],\n",
    "        bins=3,\n",
    "        attributes=input_cols,\n",
    "        strategy=\"quantile\", \n",
    "        round_v = 2,\n",
    "        min_distinct=5,\n",
    "        )\n",
    "\n",
    "    ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "    replace_values = {}\n",
    "    for i in range(0,len(signal_cols)):\n",
    "        for v in df_discretized[signal_cols[i]].unique():\n",
    "            if \"<=\" == v[0:2]:\n",
    "                replace_values[v] = \"low\"\n",
    "            elif \">\" == v[0]:\n",
    "                replace_values[v] = \"high\"\n",
    "            elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                replace_values[v] = \"medium\"\n",
    "            else:\n",
    "                raise ValueError(v)\n",
    "        df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "    \n",
    "    df_discretized.loc[df_discretized[\"location\"]==\"none_location\", \"location\"] = \"none\"\n",
    "    df_discretized.loc[df_discretized[\"object\"]==\"none_object\", \"object\"] = \"none\"\n",
    " \n",
    "    ## Create dict of Divergence df\n",
    "    df_dict[config] = df_discretized\n",
    "    fp_diver = FP_DivergenceExplorer(\n",
    "        df_discretized, \n",
    "        true_class_name=target_col, \n",
    "        class_map={\"P\":1, \"N\":0}\n",
    "        )\n",
    "    FP_fm = fp_diver.getFrequentPatternDivergence(\n",
    "        min_support=min_sup, \n",
    "        metrics=[target_metric]\n",
    "        )\n",
    "    FP_fm.rename(\n",
    "        columns=remapped_cols, \n",
    "        inplace=True\n",
    "        )\n",
    "    FP_fm = FP_fm[show_cols].copy()\n",
    "    FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
    "    FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
    "    FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
    "    FP_fm_dict[config] = FP_fm\n",
    "    fp_divergence_dict[config] = FP_Divergence(FP_fm, target_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857d9ad",
   "metadata": {},
   "source": [
    "## Divergence wav2vec 2.0 base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89cf870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_redundancy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for wav2vec 2.0 base (original)\n",
    "config = 'original'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean negative divergence for wav2vec 2.0 base (original)\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "for i in [5, 10, 20, 50]:\n",
    "    pr = FPdiv[FPdiv['d_accuracy'] <= 0].head(i).copy()\n",
    "    print(\"Mean negative divergence top\", i, \":\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean negative divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve Most Positively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy) \n",
    "pr_top = FPdiv.head(n).copy()\n",
    "pr_top[\"support\"] = pr_top[\"support\"].round(2)\n",
    "pr_top[\"#errors\"] = pr_top[\"#errors\"].astype(int)\n",
    "pr_top[\"#corrects\"] = pr_top[\"#corrects\"].astype(int)\n",
    "pr_top[\"accuracy\"] = (pr_top[\"accuracy\"]*100).round(3)\n",
    "pr_top[\"d_accuracy\"] = (pr_top[\"d_accuracy\"]*100).round(3)\n",
    "\n",
    "## Choose columns for better visualization \n",
    "pr_l_top = pr_top[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_top['itemsets'] = pr_l_top['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean positive divergence for wav2vec 2.0 base (original)\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "print(\"Total positive subgroups: \", len(FPdiv[FPdiv['d_accuracy'] > 0]))\n",
    "for i in [5, 10, 20, 50]:\n",
    "    pr = FPdiv[FPdiv['d_accuracy'] > 0].head(i).copy()\n",
    "    print(\"Mean positive divergence top\", i, \":\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].copy()\n",
    "print(\"Mean positive divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the average divergence for wav2vec 2.0 base (original)\n",
    "pr = FPdiv.copy()\n",
    "print(\"\\nMean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2609e9",
   "metadata": {},
   "source": [
    "# ITALIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f20f38",
   "metadata": {},
   "source": [
    "## Retrieve Data and Compute Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
    "from divexplorer.FP_Divergence import FP_Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05062e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns of the df file that we are going to analyze \n",
    "demo_cols = ['gender', 'age', 'region', 'nationality', 'lisp', 'education']\n",
    "\n",
    "slot_cols = ['action', 'scenario']\n",
    "\n",
    "rec_set_cols = ['environment', 'device', 'field']\n",
    "\n",
    "signal_cols = ['total_silence', 'total_duration', 'trimmed_duration', \n",
    "'n_words', 'speed_rate_word', 'speed_rate_word_trimmed'] \n",
    "\n",
    "input_cols = demo_cols + slot_cols + rec_set_cols + signal_cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.03\n",
    "\n",
    "configs = [\n",
    "    \"original\", \n",
    "    \"acq_random\",\n",
    "    \"acq_knn\",\n",
    "    \"acq_clustering\",\n",
    "    \"acq_divexplorer\",\n",
    "    \"acq_supervision\",\n",
    "    \"aug_random\",\n",
    "    \"aug_knn\",\n",
    "    \"aug_clustering\",\n",
    "    \"aug_divexplorer\",\n",
    "    \"reg_random\",\n",
    "    \"reg_knn\",\n",
    "    \"reg_clustering\",\n",
    "    \"reg_divexplorer\",\n",
    "    ]\n",
    "\n",
    "FP_fm_dict = {}\n",
    "fp_divergence_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    input_file_divexplorer = os.path.join(\\\n",
    "        os.getcwd(), \"results\", \"slu\", \"italic\", config, \"df_test.csv\")\n",
    "    df = pd.read_csv(input_file_divexplorer, index_col=0)\n",
    "\n",
    "    ## Discretize the dataframe\n",
    "    from divergence_utils import discretize\n",
    "\n",
    "    df_discretized = discretize(\n",
    "        df[input_cols+[target_col]],\n",
    "        bins=3,\n",
    "        attributes=input_cols,\n",
    "        strategy=\"quantile\", \n",
    "        round_v = 2,\n",
    "        min_distinct=5,\n",
    "        )\n",
    "\n",
    "    ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "    replace_values = {}\n",
    "    for i in range(0,len(signal_cols)):\n",
    "        for v in df_discretized[signal_cols[i]].unique():\n",
    "            if \"<=\" == v[0:2]:\n",
    "                replace_values[v] = \"low\"\n",
    "            elif \">\" == v[0]:\n",
    "                replace_values[v] = \"high\"\n",
    "            elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                replace_values[v] = \"medium\"\n",
    "            else:\n",
    "                raise ValueError(v)\n",
    "        df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "     \n",
    "    ## Create dict of Divergence df\n",
    "    df_dict[config] = df_discretized\n",
    "    fp_diver = FP_DivergenceExplorer(\n",
    "        df_discretized, \n",
    "        true_class_name=target_col, \n",
    "        class_map={\"P\":1, \"N\":0}\n",
    "        )\n",
    "    FP_fm = fp_diver.getFrequentPatternDivergence(\n",
    "        min_support=min_sup, \n",
    "        metrics=[target_metric]\n",
    "        )\n",
    "    FP_fm.rename(\n",
    "        columns=remapped_cols, \n",
    "        inplace=True\n",
    "        )\n",
    "    FP_fm = FP_fm[show_cols].copy()\n",
    "    FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
    "    FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
    "    FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
    "    FP_fm_dict[config] = FP_fm\n",
    "    fp_divergence_dict[config] = FP_Divergence(FP_fm, target_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae572985",
   "metadata": {},
   "source": [
    "## Divergence XLSR-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23def996",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_redundancy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fe99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for XLSR-300 (original)\n",
    "config = 'original'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean negative divergence for XLSR-300 (original)\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "for i in [5, 10, 20, 50]:\n",
    "    pr = FPdiv[FPdiv['d_accuracy'] <= 0].head(i).copy()\n",
    "    print(\"Mean negative divergence top\", i, \":\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean negative divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve Most Positively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy) \n",
    "pr_top = FPdiv.head(n).copy()\n",
    "pr_top[\"support\"] = pr_top[\"support\"].round(2)\n",
    "pr_top[\"#errors\"] = pr_top[\"#errors\"].astype(int)\n",
    "pr_top[\"#corrects\"] = pr_top[\"#corrects\"].astype(int)\n",
    "pr_top[\"accuracy\"] = (pr_top[\"accuracy\"]*100).round(3)\n",
    "pr_top[\"d_accuracy\"] = (pr_top[\"d_accuracy\"]*100).round(3)\n",
    "\n",
    "## Choose columns for better visualization \n",
    "pr_l_top = pr_top[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_top['itemsets'] = pr_l_top['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean positive divergence for XLSR-300 (original)\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "print(\"Total positive subgroups: \", len(FPdiv[FPdiv['d_accuracy'] > 0]))\n",
    "for i in [5, 10, 20, 50]:\n",
    "    pr = FPdiv[FPdiv['d_accuracy'] > 0].head(i).copy()\n",
    "    print(\"Mean positive divergence top\", i, \":\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].copy()\n",
    "print(\"Mean positive divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d39f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the average divergence for XLSR-300 (original)\n",
    "pr = FPdiv.copy()\n",
    "print(\"\\nMean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7e68b",
   "metadata": {},
   "source": [
    "# IEMOCAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a464cb",
   "metadata": {},
   "source": [
    "## Retrieve Data and Compute Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5961e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
    "from divexplorer.FP_Divergence import FP_Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48594098",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns of the df file that we are going to analyze \n",
    "demo_cols = ['gender']\n",
    "\n",
    "slot_cols = ['emotion']\n",
    "\n",
    "signal_cols = ['valence', 'activation', 'dominance', \n",
    "    'total_silence', 'n_pauses', 'total_duration', 'trimmed_duration']\n",
    "\n",
    "input_cols = demo_cols + slot_cols + signal_cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.03\n",
    "\n",
    "configs = [\n",
    "    \"original\", \n",
    "    \"acq_random\",\n",
    "    \"acq_knn\",\n",
    "    \"acq_clustering\",\n",
    "    \"acq_divexplorer\",\n",
    "    \"acq_supervision\",\n",
    "    \"aug_random\",\n",
    "    \"aug_knn\",\n",
    "    \"aug_clustering\",\n",
    "    \"aug_divexplorer\",\n",
    "    \"reg_random\",\n",
    "    \"reg_knn\",\n",
    "    \"reg_clustering\",\n",
    "    \"reg_divexplorer\",\n",
    "    ]\n",
    "\n",
    "FP_fm_dict = {}\n",
    "fp_divergence_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    input_file_divexplorer = os.path.join(\\\n",
    "        os.getcwd(), \"results\", \"er\", \"iemocap\", config, \"df_test.csv\")\n",
    "    df = pd.read_csv(input_file_divexplorer, index_col=0)\n",
    "\n",
    "    ## Discretize the dataframe\n",
    "    from divergence_utils import discretize\n",
    "\n",
    "    df_discretized = discretize(\n",
    "        df[input_cols+[target_col]],\n",
    "        bins=3,\n",
    "        attributes=input_cols,\n",
    "        strategy=\"quantile\", \n",
    "        round_v = 2,\n",
    "        min_distinct=5,\n",
    "        )\n",
    "\n",
    "    ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "    replace_values = {}\n",
    "    for i in range(0,len(signal_cols)):\n",
    "        for v in df_discretized[signal_cols[i]].unique():\n",
    "            if \"<=\" == v[0:2]:\n",
    "                replace_values[v] = \"low\"\n",
    "            elif \">\" == v[0]:\n",
    "                replace_values[v] = \"high\"\n",
    "            elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                replace_values[v] = \"medium\"\n",
    "            else:\n",
    "                raise ValueError(v)\n",
    "        df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "     \n",
    "    ## Create dict of Divergence df\n",
    "    df_dict[config] = df_discretized\n",
    "    fp_diver = FP_DivergenceExplorer(\n",
    "        df_discretized, \n",
    "        true_class_name=target_col, \n",
    "        class_map={\"P\":1, \"N\":0}\n",
    "        )\n",
    "    FP_fm = fp_diver.getFrequentPatternDivergence(\n",
    "        min_support=min_sup, \n",
    "        metrics=[target_metric]\n",
    "        )\n",
    "    FP_fm.rename(\n",
    "        columns=remapped_cols, \n",
    "        inplace=True\n",
    "        )\n",
    "    FP_fm = FP_fm[show_cols].copy()\n",
    "    FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
    "    FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
    "    FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
    "    FP_fm_dict[config] = FP_fm\n",
    "    fp_divergence_dict[config] = FP_Divergence(FP_fm, target_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d72fec",
   "metadata": {},
   "source": [
    "## Divergence wav2vec 2.0 base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db98910",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_redundancy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for wav2vec 2.0 base (original)\n",
    "config = 'original'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean negative divergence for wav2vec 2.0 base (original)\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "for i in [5, 10, 20, 50]:\n",
    "    pr = FPdiv[FPdiv['d_accuracy'] <= 0].head(i).copy()\n",
    "    print(\"Mean negative divergence top\", i, \":\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean negative divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e25855",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve Most Positively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy) \n",
    "pr_top = FPdiv.head(n).copy()\n",
    "pr_top[\"support\"] = pr_top[\"support\"].round(2)\n",
    "pr_top[\"#errors\"] = pr_top[\"#errors\"].astype(int)\n",
    "pr_top[\"#corrects\"] = pr_top[\"#corrects\"].astype(int)\n",
    "pr_top[\"accuracy\"] = (pr_top[\"accuracy\"]*100).round(3)\n",
    "pr_top[\"d_accuracy\"] = (pr_top[\"d_accuracy\"]*100).round(3)\n",
    "\n",
    "## Choose columns for better visualization \n",
    "pr_l_top = pr_top[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_top['itemsets'] = pr_l_top['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean positive divergence for wav2vec 2.0 base (original)\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "print(\"Total positive subgroups: \", len(FPdiv[FPdiv['d_accuracy'] > 0]))\n",
    "for i in [5, 10, 20, 50]:\n",
    "    pr = FPdiv[FPdiv['d_accuracy'] > 0].head(i).copy()\n",
    "    print(\"Mean positive divergence top\", i, \":\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].copy()\n",
    "print(\"Mean positive divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53772294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the average divergence for wav2vec 2.0 base (original)\n",
    "pr = FPdiv.copy()\n",
    "print(\"\\nMean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8936e9",
   "metadata": {},
   "source": [
    "# LibriSpeech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e22f1",
   "metadata": {},
   "source": [
    "## Retrieve Data and Compute Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaca0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
    "from divexplorer.FP_Divergence import FP_Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target for DivExplorer: 'WER'\n",
    "target_col = 'WER' \n",
    "target_metric = 'd_outcome'\n",
    "target_div = f'd_{target_col}'\n",
    "t_value_col = 't_value_outcome'\n",
    "printable_columns = ['support', 'itemsets','WER', 'd_WER', 't_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns for visualization\n",
    "remapped_cols = { \"outcome\": target_col, \"d_outcome\": target_div, t_value_col: 't_value'}\n",
    "show_cols = ['support', 'itemsets', target_col, target_div, 'support_count', 'length', 't_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c41f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns of the df file that we are going to analyze \n",
    "demo_cols = ['gender']\n",
    "\n",
    "signal_cols = ['total_silence', 'total_duration', 'trimmed_duration', \n",
    "       'n_pauses', 'n_words', 'speed_rate_word', 'speed_rate_word_trimmed'] \n",
    "\n",
    "input_cols = demo_cols + signal_cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.03\n",
    "\n",
    "configs = [\n",
    "    \"original\", \n",
    "    \"acq_random\",\n",
    "    \"acq_knn\",\n",
    "    \"acq_clustering\",\n",
    "    \"acq_divexplorer\",\n",
    "    \"acq_supervision\",\n",
    "    \"aug_random\",\n",
    "    \"aug_knn\",\n",
    "    \"aug_clustering\",\n",
    "    \"aug_divexplorer\",\n",
    "    \"reg_random\",\n",
    "    \"reg_knn\",\n",
    "    \"reg_clustering\",\n",
    "    \"reg_divexplorer\",\n",
    "    ]\n",
    "\n",
    "FP_fm_dict = {}\n",
    "fp_divergence_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    input_file_divexplorer = os.path.join(\\\n",
    "        os.getcwd(), \"results\", \"asr\", \"librispeech\", config, \"df_test.csv\")\n",
    "    df = pd.read_csv(input_file_divexplorer, index_col=0)\n",
    "\n",
    "    ## Discretize the dataframe\n",
    "    from divergence_utils import discretize\n",
    "\n",
    "    df_discretized = discretize(\n",
    "        df[input_cols+[target_col]],\n",
    "        bins=3,\n",
    "        attributes=input_cols,\n",
    "        strategy=\"quantile\", \n",
    "        round_v = 2,\n",
    "        min_distinct=5,\n",
    "        )\n",
    "\n",
    "    ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "    replace_values = {}\n",
    "    for i in range(0,len(signal_cols)):\n",
    "        for v in df_discretized[signal_cols[i]].unique():\n",
    "            if \"<=\" == v[0:2]:\n",
    "                replace_values[v] = \"low\"\n",
    "            elif \">\" == v[0]:\n",
    "                replace_values[v] = \"high\"\n",
    "            elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                replace_values[v] = \"medium\"\n",
    "            else:\n",
    "                raise ValueError(v)\n",
    "        df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "     \n",
    "    ## Create dict of Divergence df\n",
    "    df_dict[config] = df_discretized\n",
    "    fp_diver = FP_DivergenceExplorer(\n",
    "        df_discretized, \n",
    "        target_name=target_col\n",
    "        )\n",
    "    FP_fm = fp_diver.getFrequentPatternDivergence(\n",
    "        min_support=min_sup, \n",
    "        metrics=[target_metric]\n",
    "        )\n",
    "    FP_fm.rename(\n",
    "        columns=remapped_cols, \n",
    "        inplace=True\n",
    "        )\n",
    "    FP_fm = FP_fm[show_cols].copy()\n",
    "    FP_fm['WER'] = round(FP_fm['WER'], 5)\n",
    "    FP_fm['d_WER'] = round(FP_fm['d_WER'], 5)\n",
    "    FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
    "    FP_fm_dict[config] = FP_fm\n",
    "    fp_divergence_dict[config] = FP_Divergence(FP_fm, target_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df052f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_BASE_WER = 8.06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008cde1",
   "metadata": {},
   "source": [
    "## Divergence Whisper base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88444991",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_redundancy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for Whisper base (original)\n",
    "config = 'original'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"WER\"] = (pr_bot[\"WER\"]*100).round(3)\n",
    "pr_bot[\"d_WER\"] = ((pr_bot[\"WER\"] - WHISPER_BASE_WER)).round(3)\n",
    "\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[\"itemsets\", \"support\", \"WER\", \"d_WER\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean negative divergence for Whisper base (original)\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_WER'] >= 0]))\n",
    "for i in [5, 10, 20, 50]:\n",
    "    pr = FPdiv[FPdiv['d_WER'] >= 0].head(i).copy()\n",
    "    print(\"Mean negative divergence top\", i, \":\", round(100*pr['d_WER'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_WER'] > 0].copy()\n",
    "print(\"Mean negative divergence all:\", round(100*pr['d_WER'].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve Most Positively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_top = FPdiv.head(n).copy()\n",
    "pr_top[\"support\"] = pr_top[\"support\"].round(2)\n",
    "pr_top[\"WER\"] = (pr_top[\"WER\"]*100).round(3)\n",
    "pr_top[\"d_WER\"] = ((pr_top[\"WER\"] - WHISPER_BASE_WER)).round(3)\n",
    "\n",
    "## Choose columns for better visualization \n",
    "pr_l_top = pr_top[[\"itemsets\", \"support\", \"WER\", \"d_WER\", \"t_value\"]].copy()\n",
    "pr_l_top['itemsets'] = pr_l_top['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca77c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean positive divergence for Whisper base (original)\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "print(\"Total positive subgroups: \", len(FPdiv[FPdiv['d_WER'] < 0]))\n",
    "for i in [5, 10, 20, 50]:\n",
    "    pr = FPdiv[FPdiv['d_WER'] < 0].head(i).copy()\n",
    "    print(\"Mean positive divergence top\", i, \":\", round(100*pr['d_WER'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_WER'] < 0].copy()\n",
    "print(\"Mean positive divergence all:\", round(100*pr['d_WER'].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the average divergence for Whisper base (original)\n",
    "pr = FPdiv.copy()\n",
    "print(\"\\nMean divergence all:\", round(100*pr['d_WER'].mean(), 3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DivExplorer_FSC_IC.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('speech': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "313.76837158203125px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "50f798c039f92e39594af06ec0119751541d975fa6ec3b2f5528645cd2e370ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
